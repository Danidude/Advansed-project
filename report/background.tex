\chapter{Background}
\label{ch:background}

\section{Sample stuff}
Some simple and useful latex formatting.

\subsection{Quotations and citing}
It is explained in detail in \cite[Ch.20]{Norvig03} that 
\begin{quotation}
\noindent \textit{``the true hypothesis eventually dominates the Bayesian predication. For any fixed prior that does not rule out the true hypothesis, the posterior probability of any false hypothesis will eventually vanish, simply because the probability of generating ``uncharacteristic'' data indefinitely is vanishingly small.''}
\end{quotation}
\subsection{Figures}
This distribution, and its probability density function, is displayed in Figure \ref{fig:gaussian_distr_pdf}.
\begin{figure}[ht]
	\center\includegraphics[width=10cm]{images/normal_distr_pdf}	
	\label{fig:gaussian_distr_pdf}
	\caption{The Normal distribution PDF.}
\end{figure}

\subsection{Equations}
By using these probabilities, and Bayes formula, we can derive the Bayes classifier.
\begin{equation}
	P(\omega_i | \boldsymbol{x}, \mathcal{X}) = \frac{p(\boldsymbol{x}|\omega_i, \mathcal(X))P(\omega_i|\mathcal{X})}{\sum_{j=1}^{c}p(\boldsymbol{x}|\omega_j, \mathcal{X})P(\omega_j|\mathcal{X})},
	\label{eq:bayes_formula_1}
\end{equation}
when we can separate the training samples by class into $c$ subsets $\mathcal{X}_1, \ldots, \mathcal{X}_c$, with the samples in $\mathcal{X}_i$ belonging to $\omega_i$.

